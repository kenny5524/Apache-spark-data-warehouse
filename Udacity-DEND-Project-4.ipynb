{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity DEND Project-4: Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config to envs and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKIAQZ6MRHNVQALAFJWU\n",
      "9+i08D7oFfO8jPwMgrc7+99nIhDfZtZ3MvAaCVSn\n",
      "s3a://udacity-dend/\n",
      "s3a://sundog-spark12/kenny_out/\n",
      "data/song_data/A/A/A/TRAAAAW128F429D538.json\n",
      "data/log_data/2018/11/2018-11-01-events.json\n",
      "C:/Users/Kenny/farzer/project_4/outlaw/\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "AWS_ACCESS_KEY_ID=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "INPUT_DATA = config['AWS']['INPUT_DATA']\n",
    "AWS_SECRET_ACCESS_KEY= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "INPUT_DATA_SD = config['AWS']['INPUT_DATA_SD']\n",
    "INPUT_DATA_LD = config['AWS']['INPUT_DATA_LD']\n",
    "OUTPUT_DATA = config['AWS']['OUTPUT_DATA']\n",
    "SONG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_SD_LOCAL']\n",
    "LOG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_LD_LOCAL']\n",
    "OUTPUT_DATA_LOCAL=config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "\n",
    "print(AWS_ACCESS_KEY_ID)\n",
    "print(AWS_SECRET_ACCESS_KEY)\n",
    "print(INPUT_DATA)\n",
    "print(OUTPUT_DATA)\n",
    "print(SONG_DATA_LOCAL)\n",
    "print(LOG_DATA_LOCAL)\n",
    "print(OUTPUT_DATA_LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load song_data (from JSON to Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local song_data\n",
    "song_data_path = SONG_DATA_LOCAL\n",
    "\n",
    "# Use this instead if you want to read song_data from S3.\n",
    "#song_data_path = INPUT_DATA_SD\n",
    "\n",
    "df_sd = spark.read.json(song_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+---------------+----------------+-----------+---------+---------+------------------+----------------+----+\n",
      "|artist_id         |artist_latitude|artist_location|artist_longitude|artist_name|duration |num_songs|song_id           |title           |year|\n",
      "+------------------+---------------+---------------+----------------+-----------+---------+---------+------------------+----------------+----+\n",
      "|ARD7TVE1187B99BFB1|null           |California - LA|null            |Casual     |218.93179|1        |SOMZWCG12A8C13C480|I Didn't Mean To|0   |\n",
      "+------------------+---------------+---------------+----------------+-----------+---------+---------+------------------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sd.printSchema()\n",
    "df_sd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (songs_table + artists_table) Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Songs table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n",
      "+------------------+----------------+------------------+----+---------+\n",
      "|           song_id|           title|         artist_id|year| duration|\n",
      "+------------------+----------------+------------------+----+---------+\n",
      "|SOMZWCG12A8C13C480|I Didn't Mean To|ARD7TVE1187B99BFB1|   0|218.93179|\n",
      "+------------------+----------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sd.createOrReplaceTempView(\"songs_table_DF\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM songs_table_DF\n",
    "    ORDER BY song_id\n",
    "\"\"\")\n",
    "songs_table.printSchema()\n",
    "songs_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sundog-spark12/kenny_out/songs_table_2021-12-20-23-02-26-256716\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songs_table_path = OUTPUT_DATA + \"songs_table\" + \"_\" + now\n",
    "print(songs_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sundog-spark12/kenny_out/songs_table.parquet_2021-12-20-23-03-09-531481\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path s3a://sundog-spark12/kenny_out/songs_table.parquet_2021-12-20-23-03-09-531481 already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f5164c6afab8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Write DF to Spark parquet file (partitioned by year and artist_id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0msongs_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"artist_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msongs_table_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    937\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path s3a://sundog-spark12/kenny_out/songs_table.parquet_2021-12-20-23-03-09-531481 already exists.;"
     ]
    }
   ],
   "source": [
    "# Write DF data to JSON file(s)\n",
    "# Ref: https://stackoverflow.com/questions/29908892/save-a-large-spark-dataframe-as-a-single-json-file-in-s3\n",
    "df_sd.write.mode('append').json(songs_table_path)\n",
    "# -------\n",
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "# Partitioning: https://stackoverflow.com/questions/43731679/how-to-save-a-partitioned-parquet-file-in-spark-2-1\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songs_table_path = OUTPUT_DATA_LOCAL + \"songs_table.parquet\" + \"_\" + now\n",
    "\n",
    "# Use this instead if you want to store output to S3.\n",
    "songs_table_path = OUTPUT_DATA + \"songs_table.parquet\" + \"_\" + now\n",
    "print(songs_table_path)\n",
    "\n",
    "# NOTE: this command doesn't have partitioning!!\n",
    "songs_table.write.parquet(songs_table_path)\n",
    "\n",
    "# Write DF to Spark parquet file (partitioned by year and artist_id)\n",
    "songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(songs_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Artists table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n",
      "+------------------+------+---------------+--------+---------+\n",
      "|artist_id         |name  |location       |latitude|longitude|\n",
      "+------------------+------+---------------+--------+---------+\n",
      "|ARD7TVE1187B99BFB1|Casual|California - LA|null    |null     |\n",
      "+------------------+------+---------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sd.createOrReplaceTempView(\"artists_table_DF\")\n",
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT  artist_id        AS artist_id, \n",
    "            artist_name      AS name, \n",
    "            artist_location  AS location, \n",
    "            artist_latitude  AS latitude, \n",
    "            artist_longitude AS longitude \n",
    "    FROM artists_table_DF\n",
    "    ORDER BY artist_id desc\n",
    "\"\"\")\n",
    "artists_table.printSchema()\n",
    "artists_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "artists_table_path = OUTPUT_DATA_LOCAL + \"artists_table.parquet\" + \"_\" + now\n",
    "\n",
    "# Use this instead if you want to store output to S3.\n",
    "artists_table_path = OUTPUT_DATA + \"artists_table.parquet\" + \"_\" + now\n",
    "\n",
    "#print(artists_table_path)\n",
    "songs_table.write.parquet(artists_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load log_data (from JSON to Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local song_data\n",
    "df_ld = spark.read.json(LOG_DATA_LOCAL)\n",
    "\n",
    "# Use this instead if you want to read log_data from S3.\n",
    "df_ld = spark.read.json(INPUT_DATA_LD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld.printSchema()\n",
    "df_ld.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_filtered = df_ld.filter(df_ld.page == 'NextSong')\n",
    "df_ld_filtered.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (users_table + time_table + songplays_table) Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Users table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_filtered.createOrReplaceTempView(\"users_table_DF\")\n",
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT userId    AS user_id, \n",
    "                     firstName AS first_name, \n",
    "                     lastName  AS last_name, \n",
    "                     gender, \n",
    "                     level\n",
    "    FROM users_table_DF\n",
    "    ORDER BY last_name\n",
    "\"\"\")\n",
    "users_table.printSchema()\n",
    "users_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "users_table_path = OUTPUT_DATA_LOCAL + \"users_table.parquet\" + \"_\" + now\n",
    "print(users_table_path)\n",
    "users_table.write.parquet(users_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Time table + write it to parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with timestamp\n",
    "# \n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as t\n",
    "#from datetime import datetime\n",
    "\n",
    "@udf(t.TimestampType())\n",
    "def get_timestamp (ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0)\n",
    "    \n",
    "df_ld_filtered = df_ld_filtered.withColumn(\"timestamp\", get_timestamp(\"ts\"))\n",
    "                    \n",
    "\n",
    "df_ld_filtered.printSchema()\n",
    "df_ld_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with datetime\n",
    "# Ref: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dateformat#pyspark.sql.functions.from_unixtime\n",
    "@udf(t.StringType())\n",
    "def get_datetime(ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_ld_filtered = df_ld_filtered.withColumn(\"datetime\", get_datetime(\"ts\"))\n",
    "df_ld_filtered.printSchema()\n",
    "df_ld_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://docs-snaplogic.atlassian.net/wiki/spaces/SD/pages/2458071/Date+Functions+and+Properties+Spark+SQL\n",
    "\n",
    "df_ld_filtered.createOrReplaceTempView(\"time_table_DF\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT datetime AS start_time, \n",
    "                     hour(timestamp) AS hour, \n",
    "                     day(timestamp)  AS day, \n",
    "                     weekofyear(timestamp) AS week,\n",
    "                     month(timestamp) AS month,\n",
    "                     year(timestamp) AS year,\n",
    "                     dayofweek(timestamp) AS weekday\n",
    "    FROM time_table_DF\n",
    "    ORDER BY start_time\n",
    "\"\"\")\n",
    "time_table.printSchema()\n",
    "time_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "time_table_path = OUTPUT_DATA_LOCAL + \"time_table.parquet\" + \"_\" + now\n",
    "print(time_table_path)\n",
    "time_table.write.parquet(time_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create songplays table + write it to parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join song_data and log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/questions/33745964/how-to-join-on-multiple-columns-in-pyspark\n",
    "df_ld_sd_joined = df_ld_filtered.join(df_sd, (df_ld_filtered.artist == df_sd.artist_name) & (df_ld_filtered.song == df_sd.title))\n",
    "df_ld_sd_joined.printSchema()\n",
    "df_ld_sd_joined.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_sd_joined = df_ld_sd_joined.withColumn(\"songplay_id\", f.monotonically_increasing_id())\n",
    "\n",
    "\n",
    "df_ld_sd_joined.createOrReplaceTempView(\"songplays_table_DF\")\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "    SELECT  songplay_id AS songplay_id, \n",
    "            timestamp   AS start_time, \n",
    "            userId      AS user_id, \n",
    "            level       AS level,\n",
    "            song_id     AS song_id,\n",
    "            artist_id   AS artist_id,\n",
    "            sessionId   AS session_id,\n",
    "            location    AS location,\n",
    "            userAgent   AS user_agent\n",
    "    FROM songplays_table_DF\n",
    "    ORDER BY (user_id, session_id) \n",
    "\"\"\")\n",
    "\n",
    "songplays_table.printSchema()\n",
    "songplays_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songplays_table_path = OUTPUT_DATA_LOCAL + \"songplays_table.parquet\" + \"_\" + now\n",
    "print(songplays_table_path)\n",
    "time_table.write.parquet(songplays_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "                     )\n",
    "song_data_path = \"udacity-dend\"\n",
    "log_data_path = INPUT_DATA + \"log_data/\"\n",
    "print(song_data_path)\n",
    "print(log_data_path)\n",
    "\n",
    "input_data_bucket =  s3.Bucket(song_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sd = 0\n",
    "for obj in input_data_bucket.objects.filter(Prefix=\"song_data\"):\n",
    "    count_sd += 1\n",
    "    print(obj)\n",
    "print(count_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ld = 0\n",
    "for obj in input_data_bucket.objects.filter(Prefix=\"log_data\"):\n",
    "    count_ld += 1\n",
    "    print(obj)\n",
    "print(count_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from parque files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read songs_table\n",
    "input_data_parquet = OUTPUT_DATA_LOCAL + \"songplays_table.parquet\"\n",
    "df_sd = spark.read.parquet(input_data_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
