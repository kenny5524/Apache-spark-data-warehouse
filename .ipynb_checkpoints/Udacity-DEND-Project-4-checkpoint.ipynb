{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity DEND Project-4: Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config to envs and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKIAQZ6MRHNVQALAFJWU\n",
      "9+i08D7oFfO8jPwMgrc7+99nIhDfZtZ3MvAaCVSn\n",
      "s3a://udacity-dend/\n",
      "s3a://sundog-spark12/kenny_out/\n",
      "data/song_data/A/A/A/TRAAAAW128F429D538.json\n",
      "data/log_data/2018/11/2018-11-01-events.json\n",
      "C:/Users/Kenny/farzer/project_4/outlaw/\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "AWS_ACCESS_KEY_ID=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "INPUT_DATA = config['AWS']['INPUT_DATA']\n",
    "AWS_SECRET_ACCESS_KEY= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "INPUT_DATA_SD = config['AWS']['INPUT_DATA_SD']\n",
    "INPUT_DATA_LD = config['AWS']['INPUT_DATA_LD']\n",
    "OUTPUT_DATA = config['AWS']['OUTPUT_DATA']\n",
    "SONG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_SD_LOCAL']\n",
    "LOG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_LD_LOCAL']\n",
    "OUTPUT_DATA_LOCAL=config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "\n",
    "print(AWS_ACCESS_KEY_ID)\n",
    "print(AWS_SECRET_ACCESS_KEY)\n",
    "print(INPUT_DATA)\n",
    "print(OUTPUT_DATA)\n",
    "print(SONG_DATA_LOCAL)\n",
    "print(LOG_DATA_LOCAL)\n",
    "print(OUTPUT_DATA_LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-eafb78986e1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.jars.packages\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"org.apache.hadoop:hadoop-aws:3.2.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[0;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misEncryptionEnabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SPARK_AUTH_SOCKET_TIMEOUT\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPythonAuthSocketTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SPARK_BUFFER_SIZE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSparkBufferSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1533\u001b[0m                     answer, self._gateway_client, self._fqn, name)\n\u001b[0;32m   1534\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m             raise Py4JError(\n\u001b[0m\u001b[0;32m   1536\u001b[0m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001b[0;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load song_data (from JSON to Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local song_data\n",
    "song_data_path = SONG_DATA_LOCAL\n",
    "\n",
    "# Use this instead if you want to read song_data from S3.\n",
    "#song_data_path = INPUT_DATA_SD\n",
    "\n",
    "df_sd = spark.read.json(song_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd.printSchema()\n",
    "df_sd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (songs_table + artists_table) Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Songs table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd.createOrReplaceTempView(\"songs_table_DF\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM songs_table_DF\n",
    "    ORDER BY song_id\n",
    "\"\"\")\n",
    "songs_table.printSchema()\n",
    "songs_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songs_table_path = OUTPUT_DATA + \"songs_table\" + \"_\" + now\n",
    "print(songs_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF data to JSON file(s)\n",
    "# Ref: https://stackoverflow.com/questions/29908892/save-a-large-spark-dataframe-as-a-single-json-file-in-s3\n",
    "#df_sd.write.mode('append').json(songs_table_path)\n",
    "# -------\n",
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "# Partitioning: https://stackoverflow.com/questions/43731679/how-to-save-a-partitioned-parquet-file-in-spark-2-1\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songs_table_path = OUTPUT_DATA_LOCAL + \"songs_table.parquet\" + \"_\" + now\n",
    "\n",
    "# Use this instead if you want to store output to S3.\n",
    "#songs_table_path = OUTPUT_DATA + \"songs_table.parquet\" + \"_\" + now\n",
    "#print(songs_table_path)\n",
    "\n",
    "# NOTE: this command doesn't have partitioning!!\n",
    "#songs_table.write.parquet(songs_table_path)\n",
    "\n",
    "# Write DF to Spark parquet file (partitioned by year and artist_id)\n",
    "songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(songs_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Artists table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd.createOrReplaceTempView(\"artists_table_DF\")\n",
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT  artist_id        AS artist_id, \n",
    "            artist_name      AS name, \n",
    "            artist_location  AS location, \n",
    "            artist_latitude  AS latitude, \n",
    "            artist_longitude AS longitude \n",
    "    FROM artists_table_DF\n",
    "    ORDER BY artist_id desc\n",
    "\"\"\")\n",
    "artists_table.printSchema()\n",
    "artists_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "artists_table_path = OUTPUT_DATA_LOCAL + \"artists_table.parquet\" + \"_\" + now\n",
    "\n",
    "# Use this instead if you want to store output to S3.\n",
    "#artists_table_path = OUTPUT_DATA + \"artists_table.parquet\" + \"_\" + now\n",
    "\n",
    "#print(artists_table_path)\n",
    "songs_table.write.parquet(artists_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load log_data (from JSON to Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local song_data\n",
    "df_ld = spark.read.json(LOG_DATA_LOCAL)\n",
    "\n",
    "# Use this instead if you want to read log_data from S3.\n",
    "#df_ld = spark.read.json(INPUT_DATA_LD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld.printSchema()\n",
    "df_ld.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_filtered = df_ld.filter(df_ld.page == 'NextSong')\n",
    "df_ld_filtered.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (users_table + time_table + songplays_table) Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Users table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_filtered.createOrReplaceTempView(\"users_table_DF\")\n",
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT userId    AS user_id, \n",
    "                     firstName AS first_name, \n",
    "                     lastName  AS last_name, \n",
    "                     gender, \n",
    "                     level\n",
    "    FROM users_table_DF\n",
    "    ORDER BY last_name\n",
    "\"\"\")\n",
    "users_table.printSchema()\n",
    "users_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "users_table_path = OUTPUT_DATA_LOCAL + \"users_table.parquet\" + \"_\" + now\n",
    "print(users_table_path)\n",
    "users_table.write.parquet(users_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Time table + write it to parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with timestamp\n",
    "# \n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as t\n",
    "#from datetime import datetime\n",
    "\n",
    "@udf(t.TimestampType())\n",
    "def get_timestamp (ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0)\n",
    "    \n",
    "df_ld_filtered = df_ld_filtered.withColumn(\"timestamp\", get_timestamp(\"ts\"))\n",
    "                    \n",
    "\n",
    "df_ld_filtered.printSchema()\n",
    "df_ld_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with datetime\n",
    "# Ref: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dateformat#pyspark.sql.functions.from_unixtime\n",
    "@udf(t.StringType())\n",
    "def get_datetime(ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_ld_filtered = df_ld_filtered.withColumn(\"datetime\", get_datetime(\"ts\"))\n",
    "df_ld_filtered.printSchema()\n",
    "df_ld_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://docs-snaplogic.atlassian.net/wiki/spaces/SD/pages/2458071/Date+Functions+and+Properties+Spark+SQL\n",
    "\n",
    "df_ld_filtered.createOrReplaceTempView(\"time_table_DF\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT datetime AS start_time, \n",
    "                     hour(timestamp) AS hour, \n",
    "                     day(timestamp)  AS day, \n",
    "                     weekofyear(timestamp) AS week,\n",
    "                     month(timestamp) AS month,\n",
    "                     year(timestamp) AS year,\n",
    "                     dayofweek(timestamp) AS weekday\n",
    "    FROM time_table_DF\n",
    "    ORDER BY start_time\n",
    "\"\"\")\n",
    "time_table.printSchema()\n",
    "time_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "time_table_path = OUTPUT_DATA_LOCAL + \"time_table.parquet\" + \"_\" + now\n",
    "print(time_table_path)\n",
    "time_table.write.parquet(time_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create songplays table + write it to parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join song_data and log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/questions/33745964/how-to-join-on-multiple-columns-in-pyspark\n",
    "df_ld_sd_joined = df_ld_filtered.join(df_sd, (df_ld_filtered.artist == df_sd.artist_name) & (df_ld_filtered.song == df_sd.title))\n",
    "df_ld_sd_joined.printSchema()\n",
    "df_ld_sd_joined.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_sd_joined = df_ld_sd_joined.withColumn(\"songplay_id\", f.monotonically_increasing_id())\n",
    "\n",
    "\n",
    "df_ld_sd_joined.createOrReplaceTempView(\"songplays_table_DF\")\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "    SELECT  songplay_id AS songplay_id, \n",
    "            timestamp   AS start_time, \n",
    "            userId      AS user_id, \n",
    "            level       AS level,\n",
    "            song_id     AS song_id,\n",
    "            artist_id   AS artist_id,\n",
    "            sessionId   AS session_id,\n",
    "            location    AS location,\n",
    "            userAgent   AS user_agent\n",
    "    FROM songplays_table_DF\n",
    "    ORDER BY (user_id, session_id) \n",
    "\"\"\")\n",
    "\n",
    "songplays_table.printSchema()\n",
    "songplays_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songplays_table_path = OUTPUT_DATA_LOCAL + \"songplays_table.parquet\" + \"_\" + now\n",
    "print(songplays_table_path)\n",
    "time_table.write.parquet(songplays_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "                     )\n",
    "song_data_path = \"udacity-dend\"\n",
    "log_data_path = INPUT_DATA + \"log_data/\"\n",
    "print(song_data_path)\n",
    "print(log_data_path)\n",
    "\n",
    "input_data_bucket =  s3.Bucket(song_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sd = 0\n",
    "for obj in input_data_bucket.objects.filter(Prefix=\"song_data\"):\n",
    "    count_sd += 1\n",
    "    print(obj)\n",
    "print(count_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ld = 0\n",
    "for obj in input_data_bucket.objects.filter(Prefix=\"log_data\"):\n",
    "    count_ld += 1\n",
    "    print(obj)\n",
    "print(count_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from parque files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read songs_table\n",
    "input_data_parquet = OUTPUT_DATA_LOCAL + \"songplays_table.parquet\"\n",
    "df_sd = spark.read.parquet(input_data_parquet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
